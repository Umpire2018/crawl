from pathlib import Path
from models import DocPage, DocSection, DocBlock
from pydantic import BaseModel
from typing import List, Union
import asyncio
from loguru import logger


class DocSentenceProcessed(BaseModel):
    """Represents a sentence after processing references to a list of strings."""

    id: str
    text: str
    references: List[str]  # Converted from List[CitationData] to List[str]


class DocBlockProcessed(BaseModel):
    sentences: List[DocSentenceProcessed]


class DocSectionProcessed(BaseModel):
    id: str
    title: str
    content: List[
        Union["DocSectionProcessed", DocBlockProcessed]
    ]  # Allow nested structure


class DocPageProcessed(BaseModel):
    """Processed document where references are stored as List[str]."""

    title: str
    content: List[DocSectionProcessed]


async def transform_content(content) -> Union[DocSectionProcessed, DocBlockProcessed]:
    """
    Recursively process DocSection and DocBlock content, converting references to List[str].
    """
    if isinstance(content, DocSection):
        return DocSectionProcessed(
            id=content.id,
            title=content.title,
            content=[await transform_content(item) for item in content.content],
        )
    elif isinstance(content, DocBlock):
        return DocBlockProcessed(
            sentences=[
                DocSentenceProcessed(
                    id=sentence.id,
                    text=sentence.text,
                    references=[
                        ref.url for ref in sentence.references if ref.status_code == 200
                    ],
                )
                for sentence in content.sentences
            ]
        )



async def process_single_file(input_file: Path, output_dir: Path):
    """
    å¤„ç†å•ä¸ª JSON æ–‡ä»¶ï¼Œè½¬æ¢å¼•ç”¨å¹¶ä¿å­˜åˆ° `final/` ç›®å½•ã€‚
    """
    # ç”Ÿæˆå»æ‰ `_url_test` åç¼€çš„æœ€ç»ˆæ–‡ä»¶å
    output_file = output_dir / (input_file.stem.replace("_url_test", "") + ".json")

    # **æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼Œé¿å…é‡å¤**
    if output_file.exists():
        logger.info(
            f"ğŸ“Œ Skipping {input_file.name} (already processed as {output_file.name})"
        )
        return

    # è¯»å– JSON æ–‡ä»¶
    with open(input_file, "r", encoding="utf-8") as f:
        doc_json = f.read()

    # è§£æä¸º DocPage
    doc_page = DocPage.model_validate_json(doc_json)

    # è½¬æ¢å†…å®¹ç»“æ„
    processed_content = [
        await transform_content(section) for section in doc_page.content
    ]

    # ç”Ÿæˆå¤„ç†åçš„ DocPageProcessed å¯¹è±¡
    doc_page_processed = DocPageProcessed(
        title=doc_page.title, content=processed_content
    )

    # ä¿å­˜è½¬æ¢åçš„ JSON æ–‡ä»¶
    with open(output_file, "w", encoding="utf-8") as fw:
        fw.write(doc_page_processed.model_dump_json(indent=2))

    logger.success(f"âœ… Processed and saved: {output_file}")


async def process_json_files(
    input_dir: Path = Path("processed"), output_dir: Path = Path("final")
):
    """
    å¤„ç†æ‰€æœ‰ `url_test.json` ç»“å°¾çš„ JSON æ–‡ä»¶ï¼Œå¹¶ä¿å­˜åˆ° `final/` ç›®å½•ï¼Œå»æ‰ `_url_test` åç¼€ã€‚
    """
    output_dir.mkdir(parents=True, exist_ok=True)  # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨

    # åªé€‰å–æ–‡ä»¶ååŒ…å« `_url_test.json` çš„ JSON æ–‡ä»¶
    input_files = [file for file in input_dir.glob("*.json") if "url_test" in file.stem]

    if not input_files:
        logger.warning("âŒ No `url_test.json` files found in input directory.")
        return

    tasks = [process_single_file(input_file, output_dir) for input_file in input_files]

    await asyncio.gather(*tasks)  # å¹¶å‘å¤„ç†æ–‡ä»¶

    logger.info("âœ… Finished processing all `url_test.json` files.")


# Example execution
if __name__ == "__main__":
    asyncio.run(process_json_files())
